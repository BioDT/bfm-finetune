{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atrantas/github/bfm-finetune/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "STORAGE_DIR: /projects/prjs1134/data/projects/biodt/storage\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from dataloaders.aurora_dataload import LargeClimateDataset, aurora_batch_collate\n",
    "from utils import seed_everything\n",
    "from bfm_finetune.paths import REPO_FOLDER, STORAGE_DIR\n",
    "\n",
    "from aurora import Aurora, Batch, Metadata, rollout\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../bfm-model/bfm_model/bfm/configs\n",
      "We scale the dataset True with normalize\n",
      "Reading test data from : /projects/prjs1134/data/projects/biodt/storage/monthly_batches/batches\n"
     ]
    }
   ],
   "source": [
    "# Batch path (overriden) # bfm_cfg.evaluation.test_data\n",
    "test_data_path = str(STORAGE_DIR / \"monthly_batches\" / \"batches\") # COMPLETE BATCHES\n",
    "stats_path = str(\n",
    "    STORAGE_DIR\n",
    "    / \"monthly_batches\"\n",
    "    / \"statistics\"\n",
    "    / \"monthly_batches_stats_splitted_channels.json\"\n",
    ")\n",
    "\n",
    "bfm_config_path = REPO_FOLDER / \"bfm-model/bfm_model/bfm/configs\"\n",
    "cwd = Path(os.getcwd())\n",
    "bfm_config_path = str(bfm_config_path.relative_to(cwd))\n",
    "bfm_config_path = f\"../bfm-model/bfm_model/bfm/configs\"\n",
    "print(bfm_config_path)\n",
    "with initialize(version_base=None, config_path=bfm_config_path, job_name=\"test_app\"):\n",
    "    bfm_cfg = compose(config_name=\"train_config.yaml\")\n",
    "\n",
    "\n",
    "test_dataset = LargeClimateDataset(\n",
    "    data_dir=test_data_path,\n",
    "    scaling_settings=bfm_cfg.data.scaling,\n",
    "    num_species=28,\n",
    "    atmos_levels=bfm_cfg.data.atmos_levels,\n",
    "    model_patch_size=bfm_cfg.model.patch_size,\n",
    "    max_files=1, # TODO Change this to iterate over the whole dataset!\n",
    "    mode=\"finetune\"\n",
    ")\n",
    "# print(test_dataset[0])\n",
    "print(\"Reading test data from :\", test_data_path)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    collate_fn = aurora_batch_collate,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of samples 1\n",
      "z torch.Size([1, 2, 13, 160, 280])\n",
      "t torch.Size([1, 2, 13, 160, 280])\n",
      "u torch.Size([1, 2, 13, 160, 280])\n",
      "v torch.Size([1, 2, 13, 160, 280])\n",
      "q torch.Size([1, 2, 13, 160, 280])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total length of samples {len(test_dataset)}\")\n",
    "batch = next(iter(test_dataloader))\n",
    "for name, t in batch.atmos_vars.items():\n",
    "    print(name, t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aurora(\n",
       "  (encoder): Perceiver3DEncoder(\n",
       "    (surf_mlp): MLP(\n",
       "      (net): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (surf_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (pos_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (scale_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (lead_time_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (absolute_time_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (atmos_levels_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (surf_token_embeds): LevelPatchEmbed(\n",
       "      (weights): ParameterDict(\n",
       "          (10u): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (10v): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (2t): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (lsm): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (msl): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (slt): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (z): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (atmos_token_embeds): LevelPatchEmbed(\n",
       "      (weights): ParameterDict(\n",
       "          (q): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (t): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (u): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (v): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "          (z): Parameter containing: [torch.cuda.FloatTensor of size 512x1x2x4x4 (cuda:0)]\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (level_agg): PerceiverResampler(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (1): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2-3): 2 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (backbone): Swin3DTransformerBackbone(\n",
       "    (time_mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0): Basic3DEncoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=512, window_size=(2, 6, 12), num_heads=8\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging3D(\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Basic3DEncoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-9): 10 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=1024, window_size=(2, 6, 12), num_heads=16\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging3D(\n",
       "          (reduction): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Basic3DEncoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=2048, window_size=(2, 6, 12), num_heads=32\n",
       "              (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "              (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0): Basic3DDecoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-7): 8 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=2048, window_size=(2, 6, 12), num_heads=32\n",
       "              (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "              (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchSplitting3D(\n",
       "          (lin1): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "          (lin2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Basic3DDecoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-9): 10 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=1024, window_size=(2, 6, 12), num_heads=16\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): PatchSplitting3D(\n",
       "          (lin1): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (lin2): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Basic3DDecoderLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x Swin3DTransformerBlock(\n",
       "            (norm1): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (attn): WindowAttention(\n",
       "              dim=512, window_size=(2, 6, 12), num_heads=8\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): AdaptiveLayerNorm(\n",
       "              (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "              (ln_modulation): Sequential(\n",
       "                (0): SiLU()\n",
       "                (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Perceiver3DDecoder(\n",
       "    (level_decoder): PerceiverResampler(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): PerceiverAttention(\n",
       "            (to_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (to_kv): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (1): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2-3): 2 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (surf_heads): ParameterDict(\n",
       "        (10u): Object of type: LinearPatchReconstruction\n",
       "        (10v): Object of type: LinearPatchReconstruction\n",
       "        (2t): Object of type: LinearPatchReconstruction\n",
       "        (msl): Object of type: LinearPatchReconstruction\n",
       "      (10u): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (10v): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (2t): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (msl): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "    (atmos_heads): ParameterDict(\n",
       "        (q): Object of type: LinearPatchReconstruction\n",
       "        (t): Object of type: LinearPatchReconstruction\n",
       "        (u): Object of type: LinearPatchReconstruction\n",
       "        (v): Object of type: LinearPatchReconstruction\n",
       "        (z): Object of type: LinearPatchReconstruction\n",
       "      (q): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (t): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (u): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (v): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "      (z): LinearPatchReconstruction(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "    (atmos_levels_embed): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Aurora(use_lora=False)  # The pretrained version does not use LoRA.\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\")\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input timestamp (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2000, 2, 1, 0, 0))\n",
      "Prediction timestamp (datetime.datetime(2000, 1, 1, 6, 0), datetime.datetime(2000, 2, 1, 6, 0))\n",
      "Batch(surf_vars={'2t': tensor([[[[23.9078, 33.9475, 27.5874,  ..., 35.4730, 28.8002, 25.5126],\n",
      "          [ 3.1884, 13.2598,  9.4724,  ..., 14.6177, 10.9763, 28.4597],\n",
      "          [10.7639, 15.9552, 23.3073,  ..., 17.5337, 25.3703, 20.0836],\n",
      "          ...,\n",
      "          [ 1.5668, 10.1005,  5.5000,  ..., 11.6880,  6.9773, 24.3605],\n",
      "          [ 8.4951, 11.6789, 19.7208,  ..., 13.8539, 22.2126, 15.2720],\n",
      "          [17.0880,  6.6258, 14.1729,  ...,  8.5999, 16.5563, 18.1276]]],\n",
      "\n",
      "\n",
      "        [[[23.9355, 33.9960, 27.6492,  ..., 35.5579, 28.8837, 25.5606],\n",
      "          [ 3.2196, 13.2924,  9.5364,  ..., 14.6789, 11.0515, 28.5593],\n",
      "          [10.7540, 16.0114, 23.3586,  ..., 17.6239, 25.4440, 20.2195],\n",
      "          ...,\n",
      "          [ 1.5541, 10.0740,  5.4956,  ..., 11.6820,  7.0019, 24.4286],\n",
      "          [ 8.4499, 11.6882, 19.7144,  ..., 13.9176, 22.2232, 15.4461],\n",
      "          [17.0335,  6.6521, 14.2077,  ...,  8.6656, 16.6721, 18.2087]]]], device='cuda:0'), '10u': tensor([[[[18.1275,  8.4715,  5.9707,  ...,  6.6200,  4.6093,  5.0301],\n",
      "          [14.0759,  6.9198,  5.4429,  ...,  4.7995,  3.8729,  1.3206],\n",
      "          [ 7.8783, -0.5030,  3.1275,  ..., -2.9037,  1.3576, -3.8490],\n",
      "          ...,\n",
      "          [12.5817,  5.0290,  3.3642,  ...,  0.1101, -1.7170, -4.9507],\n",
      "          [ 6.1483, -2.4916,  1.3491,  ..., -7.0837, -3.5533, -9.6403],\n",
      "          [ 5.2838,  5.0217,  3.8321,  ...,  0.9451, -0.3345,  1.3957]]],\n",
      "\n",
      "\n",
      "        [[[18.0161,  8.3881,  5.8730,  ...,  6.5324,  4.5063,  4.9364],\n",
      "          [14.0585,  6.8896,  5.4039,  ...,  4.7768,  3.8479,  1.3259],\n",
      "          [ 7.8715, -0.5358,  3.0861,  ..., -2.9056,  1.3528, -3.8136],\n",
      "          ...,\n",
      "          [12.6617,  5.0914,  3.4176,  ...,  0.1682, -1.6886, -4.9097],\n",
      "          [ 6.2510, -2.4223,  1.4013,  ..., -7.0489, -3.5281, -9.5832],\n",
      "          [ 5.3469,  5.1015,  3.9280,  ...,  1.0226, -0.2503,  1.5042]]]], device='cuda:0'), '10v': tensor([[[[ 9.4328,  9.1276,  7.0522,  ...,  8.6588,  6.3681, 11.0432],\n",
      "          [12.5038, 12.3280,  7.2749,  ..., 11.6458,  6.6625, 10.7043],\n",
      "          [ 6.1577, 10.7462, 11.3137,  ..., 10.4623, 11.2672, 14.7301],\n",
      "          ...,\n",
      "          [15.9137, 15.3371, 10.0106,  ..., 14.3821,  9.0964, 13.0249],\n",
      "          [ 9.4023, 13.9239, 14.5537,  ..., 13.4259, 14.3079, 17.8500],\n",
      "          [11.8490, 12.1975, 13.5502,  ..., 12.3335, 13.9469, 14.2715]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4582,  9.1441,  7.0755,  ...,  8.6959,  6.4071, 11.0477],\n",
      "          [12.5143, 12.3314,  7.2514,  ..., 11.6651,  6.6481, 10.6107],\n",
      "          [ 6.1559, 10.7469, 11.3273,  ..., 10.4947, 11.3097, 14.7129],\n",
      "          ...,\n",
      "          [15.9442, 15.3585,  9.9997,  ..., 14.4046,  9.0560, 12.8930],\n",
      "          [ 9.4356, 13.9528, 14.5938,  ..., 13.4092, 14.3039, 17.7939],\n",
      "          [11.8878, 12.2260, 13.5740,  ..., 12.3073, 13.9231, 14.2181]]]], device='cuda:0'), 'msl': tensor([[[[13812.6797, 13376.9531, 13419.0938,  ..., 13668.0391, 13709.4922, 12498.0938],\n",
      "          [12813.7656, 12234.8438, 13123.6562,  ..., 12535.0234, 13443.6875, 13264.2422],\n",
      "          [12768.0234, 13156.1719, 13192.6719,  ..., 13455.1719, 13513.5859, 13434.0859],\n",
      "          ...,\n",
      "          [13342.9688, 12775.0078, 13707.3516,  ..., 12985.4688, 13948.6328, 13801.6797],\n",
      "          [13345.6172, 13815.7578, 13819.5859,  ..., 13990.1641, 14017.3516, 13983.7422],\n",
      "          [13396.3906, 14047.5703, 13939.0000,  ..., 14226.1562, 14150.2891, 13114.2578]]],\n",
      "\n",
      "\n",
      "        [[[13819.9297, 13383.1484, 13425.1953,  ..., 13672.8281, 13713.9219, 12503.9609],\n",
      "          [12819.9844, 12240.6172, 13131.7812,  ..., 12539.4062, 13450.6484, 13270.7422],\n",
      "          [12771.0156, 13159.4219, 13199.1719,  ..., 13457.1953, 13519.0703, 13440.4922],\n",
      "          ...,\n",
      "          [13350.7031, 12782.6328, 13717.0078,  ..., 12988.7422, 13952.6406, 13800.6016],\n",
      "          [13350.0547, 13820.3516, 13827.0000,  ..., 13984.2812, 14015.7500, 13981.2188],\n",
      "          [13400.7500, 14052.6328, 13946.8984,  ..., 14219.9688, 14150.5078, 13113.3672]]]], device='cuda:0')}, static_vars={'z': tensor([[0.0287, 0.0287, 0.0287,  ..., 0.0286, 0.0286, 0.0287],\n",
      "        [0.0287, 0.0287, 0.0287,  ..., 0.0287, 0.0287, 0.0287],\n",
      "        [0.0283, 0.0284, 0.0286,  ..., 0.0289, 0.0287, 0.0284],\n",
      "        ...,\n",
      "        [0.3402, 0.3290, 0.3243,  ..., 0.4485, 0.4073, 0.3677],\n",
      "        [0.3251, 0.3114, 0.3053,  ..., 0.3706, 0.3571, 0.3323],\n",
      "        [0.3181, 0.3032, 0.2957,  ..., 0.3335, 0.3206, 0.3213]], device='cuda:0'), 'lsm': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0'), 'slt': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.3333, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333,  ..., 0.3333, 0.3333, 0.3333]], device='cuda:0')}, atmos_vars={'z': tensor([[[[[   -54.0061,    -55.8396,    -30.7619,  ...,     81.8940,    105.7351,    117.9412],\n",
      "           [   -77.9913,    -93.8459,    -67.2880,  ...,     55.0164,     79.6532,    114.5555],\n",
      "           [   -88.7934,    -86.5193,    -65.9205,  ...,     67.2847,     85.9366,     80.2213],\n",
      "           ...,\n",
      "           [   186.3757,    164.5338,    187.7589,  ...,    138.0165,    168.9743,    213.3005],\n",
      "           [   171.9080,    172.9664,    192.5960,  ...,    149.3301,    178.5807,    167.9214],\n",
      "           [   159.0235,    166.7875,    172.7501,  ...,    145.1876,    159.6862,    142.2198]],\n",
      "\n",
      "          [[  6220.4883,   6218.7466,   6236.5581,  ...,   6368.8384,   6388.3589,   6404.4829],\n",
      "           [  6205.6865,   6175.5381,   6200.2588,  ...,   6336.8052,   6359.3589,   6387.0659],\n",
      "           [  6185.0220,   6185.7695,   6193.9531,  ...,   6349.6699,   6354.9268,   6358.9170],\n",
      "           ...,\n",
      "           [  6778.9492,   6754.6128,   6781.4272,  ...,   6787.4136,   6813.7837,   6868.1924],\n",
      "           [  6762.7358,   6765.3442,   6795.1030,  ...,   6798.9932,   6831.0166,   6800.4766],\n",
      "           [  6743.6387,   6760.2441,   6763.5049,  ...,   6790.1948,   6794.5566,   6761.2827]],\n",
      "\n",
      "          [[ 12971.6758,  12960.3682,  12990.9229,  ...,  13007.7412,  13027.1377,  13041.2959],\n",
      "           [ 12941.4092,  12908.2744,  12936.1719,  ...,  12965.6348,  12986.6777,  13020.0859],\n",
      "           [ 12922.1055,  12919.5156,  12943.1465,  ...,  12977.5830,  12982.6670,  12985.5703],\n",
      "           ...,\n",
      "           [ 13490.3467,  13460.6729,  13492.4023,  ...,  13498.4922,  13527.8350,  13592.1123],\n",
      "           [ 13474.0820,  13478.0801,  13518.2959,  ...,  13512.5615,  13548.6582,  13513.9844],\n",
      "           [ 13457.1416,  13482.1758,  13491.4092,  ...,  13508.8447,  13514.1738,  13473.6143]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[131260.5625, 131275.4688, 131375.3750,  ..., 132255.9688, 132342.4688, 132361.4688],\n",
      "           [131212.7500, 131129.7500, 131207.3281,  ..., 132148.3594, 132201.4219, 132351.3906],\n",
      "           [131251.4375, 131209.7812, 131295.1562,  ..., 132224.2969, 132278.7031, 132222.4531],\n",
      "           ...,\n",
      "           [133775.1562, 133717.8438, 133823.4219,  ..., 133707.1250, 133821.9531, 134078.4062],\n",
      "           [133761.6562, 133759.8281, 133938.4375,  ..., 133749.4375, 133916.0312, 133817.3125],\n",
      "           [133705.7969, 133808.5469, 133806.5156,  ..., 133820.7969, 133827.0000, 133714.1719]],\n",
      "\n",
      "          [[154908.4062, 154972.2500, 154966.3438,  ..., 155723.7812, 155719.4062, 155782.4688],\n",
      "           [154936.7344, 154854.2188, 154913.4531,  ..., 155621.3125, 155670.3906, 155737.2812],\n",
      "           [154985.8594, 154950.7031, 154950.5156,  ..., 155719.8750, 155713.0156, 155723.3906],\n",
      "           ...,\n",
      "           [156205.1094, 156125.0781, 156191.7500,  ..., 155966.4688, 156045.6562, 156147.8594],\n",
      "           [156193.4688, 156184.2188, 156204.7969,  ..., 156055.4219, 156082.9219, 156069.1719],\n",
      "           [156188.1406, 156265.8125, 156276.4219,  ..., 156208.1875, 156209.8906, 156026.4844]],\n",
      "\n",
      "          [[195792.6562, 195781.1406, 195907.0156,  ..., 196598.7969, 196753.3594, 196718.5781],\n",
      "           [195776.3750, 195692.2812, 195784.6719,  ..., 196545.1719, 196632.0938, 196779.3281],\n",
      "           [195721.8438, 195763.7500, 195807.6875,  ..., 196619.9062, 196660.5312, 196688.2656],\n",
      "           ...,\n",
      "           [196503.5625, 196414.2656, 196510.5625,  ..., 196293.4844, 196408.3750, 196561.8125],\n",
      "           [196373.6094, 196434.6250, 196478.2812,  ..., 196337.6094, 196390.9219, 196429.4688],\n",
      "           [196287.4688, 196386.3750, 196411.3594,  ..., 196287.0312, 196328.5625, 196310.1094]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[   -76.9168,    -79.6988,    -54.1660,  ...,     80.5463,    103.9165,    116.1758],\n",
      "           [  -102.2860,   -118.8690,    -92.4329,  ...,     53.5973,     77.9817,    112.5436],\n",
      "           [  -115.1490,   -113.1185,    -93.2059,  ...,     65.8842,     84.2307,     78.4890],\n",
      "           ...,\n",
      "           [   187.0947,    165.9908,    189.6924,  ...,    140.2098,    170.9760,    215.3702],\n",
      "           [   173.7217,    174.9031,    195.2070,  ...,    152.2913,    181.4827,    170.4236],\n",
      "           [   161.5978,    169.7193,    175.7997,  ...,    148.7834,    163.0644,    145.2283]],\n",
      "\n",
      "          [[  6212.9395,   6211.1372,   6231.1353,  ...,   6366.9482,   6385.9736,   6402.3916],\n",
      "           [  6197.0508,   6167.4766,   6192.9795,  ...,   6334.8931,   6357.2832,   6384.5894],\n",
      "           [  6174.0293,   6176.1797,   6184.7964,  ...,   6347.7793,   6352.6782,   6356.8491],\n",
      "           ...,\n",
      "           [  6778.7451,   6754.9985,   6782.3823,  ...,   6750.0039,   6776.7861,   6829.9810],\n",
      "           [  6763.4102,   6766.1260,   6796.4897,  ...,   6762.1938,   6792.8477,   6764.5894],\n",
      "           [  6744.8091,   6761.8501,   6765.4492,  ...,   6753.6816,   6758.7432,   6724.8892]],\n",
      "\n",
      "          [[ 13003.6514,  12991.1367,  13024.1943,  ...,  13001.9648,  13020.7598,  13035.2383],\n",
      "           [ 12969.3838,  12938.1289,  12966.7178,  ...,  12959.9297,  12980.7559,  13013.6064],\n",
      "           [ 12952.4004,  12949.3496,  12976.7305,  ...,  12971.9912,  12976.6299,  12979.7383],\n",
      "           ...,\n",
      "           [ 13490.0107,  13460.8574,  13493.0059,  ...,  13492.1494,  13522.0127,  13586.9658],\n",
      "           [ 13474.3125,  13478.5469,  13519.2041,  ...,  13506.3828,  13542.9336,  13508.9141],\n",
      "           [ 13457.8525,  13483.3057,  13492.8350,  ...,  13503.1514,  13508.8799,  13468.5566]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[131268.0156, 131285.5156, 131382.9219,  ..., 132222.5781, 132308.4688, 132327.5469],\n",
      "           [131220.4688, 131138.2812, 131216.1094,  ..., 132114.6875, 132167.5781, 132317.0625],\n",
      "           [131259.7969, 131217.3281, 131301.7031,  ..., 132191.2969, 132245.3750, 132189.2188],\n",
      "           ...,\n",
      "           [133611.4531, 133553.4531, 133657.9219,  ..., 133527.0469, 133640.1562, 133890.8438],\n",
      "           [133598.6094, 133589.9375, 133764.2031,  ..., 133572.1406, 133731.8281, 133636.2812],\n",
      "           [133539.5000, 133640.6719, 133638.0000,  ..., 133645.5312, 133650.9531, 133531.7656]],\n",
      "\n",
      "          [[154902.2969, 154968.6719, 154961.8594,  ..., 155716.0312, 155710.6719, 155774.6562],\n",
      "           [154931.3906, 154849.7812, 154910.2500,  ..., 155613.3906, 155662.5469, 155728.3594],\n",
      "           [154980.1875, 154945.2969, 154945.6250,  ..., 155711.5312, 155704.1094, 155715.6875],\n",
      "           ...,\n",
      "           [156164.8906, 156087.8281, 156155.5000,  ..., 155907.9062, 155988.7812, 156089.7969],\n",
      "           [156162.2188, 156147.1406, 156167.4062,  ..., 155997.2031, 156024.4844, 156013.4844],\n",
      "           [156162.3281, 156238.8281, 156246.0156,  ..., 156151.1250, 156154.4844, 155970.1406]],\n",
      "\n",
      "          [[195785.8438, 195777.1719, 195899.0625,  ..., 196577.4531, 196729.0469, 196696.0156],\n",
      "           [195770.8438, 195687.1250, 195779.1875,  ..., 196522.8281, 196609.1406, 196754.4375],\n",
      "           [195716.3438, 195756.9844, 195799.2344,  ..., 196597.5469, 196636.7344, 196664.7969],\n",
      "           ...,\n",
      "           [196496.4062, 196409.9531, 196507.4688,  ..., 196307.8906, 196424.7500, 196578.5156],\n",
      "           [196369.3438, 196424.9062, 196472.1875,  ..., 196348.9375, 196402.5938, 196442.0625],\n",
      "           [196282.6562, 196381.3125, 196403.5312,  ..., 196296.7812, 196338.4688, 196319.0000]]]]], device='cuda:0'), 't': tensor([[[[[287.0970, 287.3259, 287.4680,  ..., 289.8548, 289.8843, 289.8633],\n",
      "           [287.5888, 287.5512, 288.0385,  ..., 290.0629, 290.3306, 290.4550],\n",
      "           [287.3911, 287.3999, 287.6439,  ..., 289.9611, 290.1497, 290.5464],\n",
      "           ...,\n",
      "           [285.3743, 285.5280, 286.1381,  ..., 286.0522, 286.6588, 286.6417],\n",
      "           [285.2002, 285.5436, 285.8476,  ..., 285.8560, 286.2266, 286.7393],\n",
      "           [285.6591, 285.1433, 285.4445,  ..., 285.3816, 285.7485, 286.4664]],\n",
      "\n",
      "          [[281.5537, 281.7743, 281.9332,  ..., 282.5491, 282.6842, 283.0110],\n",
      "           [281.8443, 281.7554, 282.3837,  ..., 282.5898, 283.1911, 282.9333],\n",
      "           [281.5767, 281.6922, 281.9116,  ..., 282.5770, 282.8490, 283.2972],\n",
      "           ...,\n",
      "           [281.5024, 281.8325, 282.4655,  ..., 281.6825, 282.5196, 282.8592],\n",
      "           [281.4885, 281.8335, 282.2693,  ..., 281.4793, 282.1961, 282.8993],\n",
      "           [281.9513, 281.5971, 281.9979,  ..., 281.1079, 281.7868, 282.7365]],\n",
      "\n",
      "          [[277.1647, 277.2905, 277.3767,  ..., 277.4666, 277.4949, 277.5246],\n",
      "           [277.4159, 277.2876, 277.6794,  ..., 277.3365, 277.6893, 277.1693],\n",
      "           [277.2708, 277.2622, 277.3183,  ..., 277.2730, 277.2553, 277.2989],\n",
      "           ...,\n",
      "           [276.3035, 276.4558, 276.9305,  ..., 276.6094, 277.0789, 277.1967],\n",
      "           [276.2672, 276.4521, 276.7325,  ..., 276.4889, 276.8974, 277.2442],\n",
      "           [276.6468, 276.2095, 276.4618,  ..., 276.0816, 276.4834, 277.0693]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[214.8170, 214.8225, 214.8199,  ..., 215.9387, 215.9449, 216.0377],\n",
      "           [214.7136, 214.5584, 214.8024,  ..., 215.6271, 215.7878, 215.6735],\n",
      "           [214.4203, 214.4328, 214.4998,  ..., 215.4293, 215.4822, 215.6308],\n",
      "           ...,\n",
      "           [214.8932, 214.7861, 214.8918,  ..., 215.9668, 216.0509, 216.2491],\n",
      "           [214.6922, 214.5886, 214.6645,  ..., 215.7493, 215.8820, 216.0688],\n",
      "           [214.8337, 214.4798, 214.5670,  ..., 215.6775, 215.8255, 216.2178]],\n",
      "\n",
      "          [[206.6814, 206.7200, 206.7320,  ..., 207.5639, 207.5924, 207.7243],\n",
      "           [206.7018, 206.5412, 206.8391,  ..., 207.4810, 207.7095, 207.4689],\n",
      "           [206.5450, 206.5929, 206.6405,  ..., 207.6021, 207.6152, 207.7633],\n",
      "           ...,\n",
      "           [206.3161, 206.2750, 206.5741,  ..., 207.0799, 207.2656, 207.1147],\n",
      "           [206.1176, 206.1561, 206.2871,  ..., 207.0864, 207.1644, 207.3405],\n",
      "           [206.4234, 205.9881, 206.0975,  ..., 207.1544, 207.2289, 207.6684]],\n",
      "\n",
      "          [[211.0103, 210.9541, 210.7815,  ..., 211.4516, 211.2480, 211.2468],\n",
      "           [211.0187, 210.8575, 211.0088,  ..., 211.4808, 211.5951, 211.1866],\n",
      "           [210.8387, 210.9045, 210.8963,  ..., 211.6846, 211.6419, 211.6512],\n",
      "           ...,\n",
      "           [210.1429, 210.0455, 210.2521,  ..., 208.9441, 209.1307, 208.9338],\n",
      "           [210.0872, 210.1587, 210.2128,  ..., 209.1081, 209.1748, 209.2813],\n",
      "           [210.5207, 210.1823, 210.2461,  ..., 209.1202, 209.2279, 209.3971]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[286.3398, 286.6010, 286.7685,  ..., 289.7574, 289.7923, 289.7729],\n",
      "           [286.7912, 286.7819, 287.3022,  ..., 289.9532, 290.2261, 290.3481],\n",
      "           [286.5706, 286.6107, 286.8667,  ..., 289.8399, 290.0291, 290.4265],\n",
      "           ...,\n",
      "           [285.3491, 285.5025, 286.1037,  ..., 286.0300, 286.6481, 286.6272],\n",
      "           [285.1833, 285.5208, 285.8264,  ..., 285.8316, 286.2072, 286.7250],\n",
      "           [285.6230, 285.1171, 285.4233,  ..., 285.3485, 285.7176, 286.4444]],\n",
      "\n",
      "          [[280.6322, 280.8687, 281.0470,  ..., 282.5449, 282.6808, 283.0019],\n",
      "           [280.9767, 280.8957, 281.5508,  ..., 282.5899, 283.1856, 282.9252],\n",
      "           [280.7324, 280.8671, 281.0805,  ..., 282.5777, 282.8425, 283.2844],\n",
      "           ...,\n",
      "           [281.5255, 281.8524, 282.4758,  ..., 281.5099, 282.3688, 282.6263],\n",
      "           [281.5215, 281.8593, 282.2919,  ..., 281.2998, 281.9961, 282.6996],\n",
      "           [281.9828, 281.6300, 282.0314,  ..., 280.8939, 281.5605, 282.5204]],\n",
      "\n",
      "          [[277.0635, 277.2233, 277.3340,  ..., 277.3990, 277.4272, 277.4573],\n",
      "           [277.3211, 277.2546, 277.6133,  ..., 277.2715, 277.6273, 277.0992],\n",
      "           [277.2454, 277.2454, 277.3227,  ..., 277.2104, 277.1930, 277.2372],\n",
      "           ...,\n",
      "           [276.2250, 276.3753, 276.8459,  ..., 276.4548, 276.9324, 277.0423],\n",
      "           [276.1910, 276.3688, 276.6472,  ..., 276.3366, 276.7425, 277.0891],\n",
      "           [276.5701, 276.1278, 276.3768,  ..., 275.9280, 276.3239, 276.9134]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[214.8746, 214.8792, 214.8758,  ..., 215.8482, 215.8527, 215.9477],\n",
      "           [214.7778, 214.6202, 214.8554,  ..., 215.5363, 215.6982, 215.5768],\n",
      "           [214.4974, 214.5030, 214.5647,  ..., 215.3427, 215.3928, 215.5402],\n",
      "           ...,\n",
      "           [214.9529, 214.8462, 214.9429,  ..., 215.7889, 215.8762, 216.0543],\n",
      "           [214.7803, 214.6669, 214.7324,  ..., 215.5912, 215.7203, 215.9121],\n",
      "           [214.9388, 214.5733, 214.6504,  ..., 215.5276, 215.6741, 216.0739]],\n",
      "\n",
      "          [[206.7524, 206.7913, 206.8051,  ..., 207.5401, 207.5683, 207.6987],\n",
      "           [206.7643, 206.6032, 206.8964,  ..., 207.4610, 207.6888, 207.4448],\n",
      "           [206.5971, 206.6400, 206.6828,  ..., 207.5848, 207.5970, 207.7449],\n",
      "           ...,\n",
      "           [205.8917, 205.8559, 206.1492,  ..., 206.9608, 207.1370, 206.9706],\n",
      "           [205.7653, 205.8037, 205.9352,  ..., 206.9679, 207.0356, 207.2099],\n",
      "           [206.1526, 205.7095, 205.8270,  ..., 207.0354, 207.1053, 207.5418]],\n",
      "\n",
      "          [[211.0903, 211.0287, 210.8543,  ..., 211.4178, 211.2143, 211.2105],\n",
      "           [211.0941, 210.9272, 211.0715,  ..., 211.4375, 211.5497, 211.1443],\n",
      "           [210.9129, 210.9701, 210.9588,  ..., 211.6324, 211.5907, 211.6027],\n",
      "           ...,\n",
      "           [209.6766, 209.5730, 209.7700,  ..., 208.9133, 209.0979, 208.9050],\n",
      "           [209.6310, 209.6827, 209.7351,  ..., 209.0620, 209.1274, 209.2393],\n",
      "           [210.0484, 209.7111, 209.7746,  ..., 209.0636, 209.1717, 209.3442]]]]], device='cuda:0'), 'u': tensor([[[[[ 2.1746,  1.9288,  2.1147,  ...,  0.3729,  0.8254,  1.2432],\n",
      "           [ 1.9314,  1.9728,  2.0207,  ...,  0.3359,  0.6143,  1.1312],\n",
      "           [ 2.0699,  1.9979,  2.4016,  ...,  0.5167,  1.0919,  1.2650],\n",
      "           ...,\n",
      "           [ 2.7525,  2.7948,  2.7757,  ...,  1.1168,  1.2595,  1.7299],\n",
      "           [ 2.5996,  2.6514,  3.1075,  ...,  1.0591,  1.8195,  2.1865],\n",
      "           [ 1.7552,  2.1621,  2.4565,  ...,  1.0313,  1.7021,  2.3211]],\n",
      "\n",
      "          [[ 0.9413,  0.8287,  1.2279,  ..., -0.3812,  0.3655,  1.1720],\n",
      "           [ 0.8168,  1.1683,  1.4325,  ..., -0.1727,  0.3660,  1.0289],\n",
      "           [ 1.1895,  1.3679,  1.9236,  ..., -0.1443,  0.6240,  0.7763],\n",
      "           ...,\n",
      "           [ 3.6218,  3.7884,  3.9486,  ...,  1.7150,  2.1076,  2.6599],\n",
      "           [ 3.8298,  3.8759,  4.4433,  ...,  1.8254,  2.7345,  3.0727],\n",
      "           [ 3.7115,  4.0382,  4.1418,  ...,  2.4414,  3.0306,  3.6614]],\n",
      "\n",
      "          [[ 2.7250,  2.7488,  3.3482,  ...,  2.2406,  3.0908,  3.9740],\n",
      "           [ 2.8537,  3.3054,  3.6871,  ...,  3.3192,  3.8676,  4.5541],\n",
      "           [ 3.3772,  3.4961,  4.1201,  ...,  4.0792,  4.7025,  4.8143],\n",
      "           ...,\n",
      "           [ 4.4596,  4.5568,  4.5493,  ...,  3.1505,  3.2031,  3.4639],\n",
      "           [ 4.9599,  4.8889,  5.2439,  ...,  3.3110,  3.8320,  3.7596],\n",
      "           [ 4.5200,  4.7401,  4.7074,  ...,  3.3759,  3.5873,  3.6984]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[15.7206, 15.7010, 16.4947,  ..., 15.4134, 16.5155, 17.6071],\n",
      "           [16.7659, 17.4120, 18.0119,  ..., 17.4324, 18.2272, 19.3017],\n",
      "           [18.6202, 18.7791, 19.5177,  ..., 19.4068, 20.1546, 20.4806],\n",
      "           ...,\n",
      "           [20.4210, 20.1491, 19.9289,  ..., 20.9480, 20.7783, 21.1501],\n",
      "           [21.1793, 20.8193, 21.0435,  ..., 21.2272, 21.5955, 21.7492],\n",
      "           [20.3961, 20.6395, 20.5549,  ..., 21.3444, 21.6807, 22.0667]],\n",
      "\n",
      "          [[11.7577, 11.9132, 12.6153,  ..., 13.2744, 14.3045, 14.9428],\n",
      "           [12.3467, 12.7957, 13.0550,  ..., 14.6513, 15.1734, 15.9458],\n",
      "           [13.4604, 13.5143, 13.8596,  ..., 16.0562, 16.5745, 16.6944],\n",
      "           ...,\n",
      "           [12.9993, 13.0929, 13.0571,  ..., 13.6163, 13.5712, 13.8986],\n",
      "           [13.6565, 13.4728, 13.5800,  ..., 13.6826, 14.0137, 14.1413],\n",
      "           [13.3764, 13.7511, 13.7689,  ..., 14.3062, 14.7002, 15.0135]],\n",
      "\n",
      "          [[ 9.7763,  9.2295,  9.7212,  ...,  9.8017, 10.3979, 10.8361],\n",
      "           [ 8.9295,  8.9269,  9.2126,  ...,  9.6535,  9.8932, 10.6223],\n",
      "           [ 8.5103,  8.3312,  9.0099,  ...,  9.3909, 10.0264,  9.8733],\n",
      "           ...,\n",
      "           [ 7.2665,  7.0486,  7.0335,  ...,  7.6282,  7.2779,  7.5531],\n",
      "           [ 7.0454,  6.6900,  6.9712,  ...,  6.6511,  6.6695,  6.3524],\n",
      "           [ 5.8978,  6.0256,  5.9610,  ...,  5.5310,  5.2485,  5.0540]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.2854,  2.1016,  2.3498,  ...,  0.4243,  0.8785,  1.3022],\n",
      "           [ 2.1452,  2.2348,  2.3169,  ...,  0.3885,  0.6696,  1.1896],\n",
      "           [ 2.4102,  2.3572,  2.7687,  ...,  0.5716,  1.1443,  1.3169],\n",
      "           ...,\n",
      "           [ 2.7773,  2.8127,  2.7892,  ...,  1.1371,  1.2497,  1.6862],\n",
      "           [ 2.6253,  2.6721,  3.1237,  ...,  1.0902,  1.8118,  2.1376],\n",
      "           [ 1.7868,  2.1865,  2.4770,  ...,  1.0770,  1.7072,  2.2865]],\n",
      "\n",
      "          [[ 0.9072,  0.8252,  1.2456,  ..., -0.2847,  0.4615,  1.2671],\n",
      "           [ 0.7697,  1.1513,  1.4368,  ..., -0.0804,  0.4602,  1.1216],\n",
      "           [ 1.0586,  1.2592,  1.8443,  ..., -0.0421,  0.7207,  0.8686],\n",
      "           ...,\n",
      "           [ 3.6544,  3.8111,  3.9640,  ...,  1.6885,  2.0827,  2.6187],\n",
      "           [ 3.8606,  3.8972,  4.4571,  ...,  1.8013,  2.7025,  3.0293],\n",
      "           [ 3.7429,  4.0615,  4.1624,  ...,  2.4561,  3.0341,  3.6542]],\n",
      "\n",
      "          [[ 2.8338,  2.8474,  3.4676,  ...,  2.1922,  3.0426,  3.9248],\n",
      "           [ 2.9476,  3.3768,  3.7539,  ...,  3.2679,  3.8188,  4.5043],\n",
      "           [ 3.5302,  3.6251,  4.2573,  ...,  4.0248,  4.6459,  4.7570],\n",
      "           ...,\n",
      "           [ 4.4203,  4.5100,  4.4975,  ...,  3.0764,  3.1098,  3.3529],\n",
      "           [ 4.9068,  4.8303,  5.1804,  ...,  3.2336,  3.7326,  3.6412],\n",
      "           [ 4.4740,  4.6876,  4.6535,  ...,  3.2972,  3.4920,  3.5884]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[15.8076, 15.7878, 16.5744,  ..., 15.3413, 16.4466, 17.5423],\n",
      "           [16.8665, 17.5039, 18.1004,  ..., 17.3683, 18.1687, 19.2532],\n",
      "           [18.7613, 18.9135, 19.6533,  ..., 19.3808, 20.1326, 20.4660],\n",
      "           ...,\n",
      "           [20.8099, 20.5786, 20.4198,  ..., 21.2145, 21.0207, 21.3555],\n",
      "           [21.4189, 21.1197, 21.4424,  ..., 21.4742, 21.8146, 21.9406],\n",
      "           [20.6928, 20.9842, 21.0146,  ..., 21.5759, 21.8544, 22.1938]],\n",
      "\n",
      "          [[11.8147, 11.9681, 12.6689,  ..., 13.2913, 14.3173, 14.9559],\n",
      "           [12.3679, 12.8142, 13.0748,  ..., 14.6605, 15.1839, 15.9577],\n",
      "           [13.5025, 13.5532, 13.9063,  ..., 16.0682, 16.5839, 16.7038],\n",
      "           ...,\n",
      "           [12.9292, 13.0548, 13.0415,  ..., 13.7405, 13.7050, 14.0395],\n",
      "           [13.7029, 13.5635, 13.6933,  ..., 13.7634, 14.1020, 14.2398],\n",
      "           [13.5804, 13.9577, 13.9945,  ..., 14.3875, 14.7918, 15.1172]],\n",
      "\n",
      "          [[ 9.8593,  9.3369,  9.8401,  ...,  9.8775, 10.4763, 10.9152],\n",
      "           [ 9.0565,  9.0689,  9.3729,  ...,  9.7420,  9.9941, 10.7272],\n",
      "           [ 8.6644,  8.5041,  9.2027,  ...,  9.5050, 10.1480,  9.9998],\n",
      "           ...,\n",
      "           [ 7.3398,  7.1632,  7.1858,  ...,  7.4577,  7.1070,  7.3760],\n",
      "           [ 7.1231,  6.7968,  7.0986,  ...,  6.4786,  6.4865,  6.1638],\n",
      "           [ 6.0273,  6.1442,  6.0812,  ...,  5.3830,  5.0996,  4.8943]]]]], device='cuda:0'), 'v': tensor([[[[[     1.1508,      1.4695,      1.5693,  ...,      0.0554,     -0.2263,     -0.5103],\n",
      "           [     0.9719,      1.2183,      1.0702,  ...,     -0.0665,     -0.3863,     -0.6077],\n",
      "           [     0.7199,      0.7757,      0.7313,  ...,     -0.5500,     -0.6092,     -0.6103],\n",
      "           ...,\n",
      "           [     0.5363,      0.1912,     -0.3153,  ...,      0.5852,     -0.0019,     -0.5155],\n",
      "           [     0.1180,     -0.2113,     -0.5156,  ...,     -0.0036,     -0.3907,     -0.8627],\n",
      "           [    -0.2872,     -0.6749,     -0.9290,  ...,     -0.6781,     -0.9987,     -1.3709]],\n",
      "\n",
      "          [[     3.4112,      3.7569,      3.8280,  ...,      2.5381,      2.2187,      2.0730],\n",
      "           [     2.8539,      3.2363,      3.2474,  ...,      2.4967,      2.2918,      2.0972],\n",
      "           [     2.4840,      2.7083,      2.8397,  ...,      2.2905,      2.3232,      2.3747],\n",
      "           ...,\n",
      "           [     1.4133,      1.2903,      1.1495,  ...,      2.5071,      2.2267,      2.2466],\n",
      "           [     0.5237,      0.4272,      0.5856,  ...,      1.4564,      1.4274,      1.5423],\n",
      "           [    -0.0624,     -0.0733,      0.1222,  ...,      0.8804,      0.9164,      0.8432]],\n",
      "\n",
      "          [[     2.3856,      3.0229,      3.2538,  ...,      2.6947,      2.7309,      2.7898],\n",
      "           [     2.7501,      3.3158,      3.3280,  ...,      2.9800,      2.8969,      2.7174],\n",
      "           [     3.1438,      3.3217,      3.3115,  ...,      2.9332,      2.8720,      2.7536],\n",
      "           ...,\n",
      "           [     0.2817,      0.4729,      0.3982,  ...,     -0.2460,     -0.2398,     -0.0205],\n",
      "           [    -0.1567,     -0.2271,     -0.2250,  ...,     -0.9723,     -0.9469,     -0.8744],\n",
      "           [    -0.5549,     -0.6651,     -0.7863,  ...,     -1.3457,     -1.4225,     -1.6296]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[     5.3900,      5.8323,      6.2529,  ...,      6.9987,      7.0012,      6.8454],\n",
      "           [     4.9631,      5.3926,      5.3780,  ...,      7.3062,      7.2438,      7.0642],\n",
      "           [     4.3535,      4.4382,      4.3281,  ...,      6.8825,      7.0208,      6.8662],\n",
      "           ...,\n",
      "           [    -0.3027,     -0.2774,     -0.3578,  ...,     -0.7311,     -0.6552,     -0.5262],\n",
      "           [    -0.8284,     -0.9670,     -0.8712,  ...,     -1.0048,     -0.8914,     -1.0109],\n",
      "           [    -0.8503,     -0.9600,     -1.0207,  ...,     -0.7960,     -0.9082,     -1.2100]],\n",
      "\n",
      "          [[    10.6494,     11.0505,     11.2288,  ...,     11.6328,     11.6916,     11.7302],\n",
      "           [    10.7863,     10.9990,     10.8366,  ...,     11.8749,     11.7549,     11.6213],\n",
      "           [    10.6901,     10.6593,     10.4764,  ...,     11.6061,     11.5663,     11.5187],\n",
      "           ...,\n",
      "           [     5.3696,      4.9000,      4.3780,  ...,      5.1277,      5.1259,      5.1439],\n",
      "           [     5.5078,      4.9339,      4.5087,  ...,      5.5873,      5.6549,      5.6503],\n",
      "           [     5.5209,      5.0526,      4.7026,  ...,      6.0261,      6.2505,      6.2372]],\n",
      "\n",
      "          [[     3.8426,      4.0822,      4.2093,  ...,      2.7150,      2.6715,      2.8724],\n",
      "           [     4.1251,      4.3724,      4.4264,  ...,      3.4449,      3.4623,      3.6278],\n",
      "           [     4.3260,      4.4884,      4.7015,  ...,      3.9816,      4.2086,      4.6148],\n",
      "           ...,\n",
      "           [     0.2636,      0.2981,      0.3185,  ...,      0.9312,      1.0115,      1.1415],\n",
      "           [     0.5694,      0.5371,      0.7087,  ...,      1.4564,      1.6311,      1.7570],\n",
      "           [     1.0516,      1.0775,      1.1976,  ...,      2.2470,      2.3727,      2.4878]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[     1.2832,      1.5728,      1.6511,  ...,      0.1448,     -0.1402,     -0.4278],\n",
      "           [     1.0756,      1.2886,      1.1106,  ...,      0.0190,     -0.3044,     -0.5281],\n",
      "           [     0.7372,      0.7438,      0.6682,  ...,     -0.4691,     -0.5329,     -0.5360],\n",
      "           ...,\n",
      "           [     0.5878,      0.2332,     -0.2764,  ...,      0.6528,      0.0581,     -0.4717],\n",
      "           [     0.1676,     -0.1728,     -0.4794,  ...,      0.1023,     -0.2957,     -0.7854],\n",
      "           [    -0.2285,     -0.6286,     -0.8872,  ...,     -0.5447,     -0.8827,     -1.2732]],\n",
      "\n",
      "          [[     3.6929,      4.0172,      4.0640,  ...,      2.6419,      2.3207,      2.1660],\n",
      "           [     3.1347,      3.4799,      3.4442,  ...,      2.5975,      2.3890,      2.1914],\n",
      "           [     2.7615,      2.9307,      3.0075,  ...,      2.3933,      2.4210,      2.4665],\n",
      "           ...,\n",
      "           [     1.4185,      1.2909,      1.1528,  ...,      2.6820,      2.4101,      2.4261],\n",
      "           [     0.5218,      0.4262,      0.5945,  ...,      1.6101,      1.5941,      1.7090],\n",
      "           [    -0.0633,     -0.0681,      0.1425,  ...,      1.0197,      1.0792,      1.0199]],\n",
      "\n",
      "          [[     2.6048,      3.2354,      3.4575,  ...,      2.6941,      2.7242,      2.7719],\n",
      "           [     2.9704,      3.5058,      3.5070,  ...,      2.9779,      2.8899,      2.7046],\n",
      "           [     3.2820,      3.4242,      3.4125,  ...,      2.9313,      2.8656,      2.7422],\n",
      "           ...,\n",
      "           [     0.3858,      0.5803,      0.5096,  ...,     -0.1326,     -0.1332,      0.0669],\n",
      "           [    -0.0358,     -0.0989,     -0.0932,  ...,     -0.8357,     -0.8171,     -0.7603],\n",
      "           [    -0.4230,     -0.5258,     -0.6444,  ...,     -1.1948,     -1.2778,     -1.4931]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[     5.4692,      5.8983,      6.3248,  ...,      6.8136,      6.8202,      6.6729],\n",
      "           [     5.0163,      5.4339,      5.4214,  ...,      7.1139,      7.0579,      6.8888],\n",
      "           [     4.3860,      4.4644,      4.3587,  ...,      6.6911,      6.8389,      6.6955],\n",
      "           ...,\n",
      "           [    -0.6671,     -0.6286,     -0.7146,  ...,     -0.4965,     -0.3759,     -0.2431],\n",
      "           [    -1.1251,     -1.2284,     -1.1426,  ...,     -0.8682,     -0.6958,     -0.8132],\n",
      "           [    -1.1518,     -1.2145,     -1.2505,  ...,     -0.8175,     -0.8494,     -1.1218]],\n",
      "\n",
      "          [[    10.6534,     11.0505,     11.2338,  ...,     11.5486,     11.6002,     11.6261],\n",
      "           [    10.7990,     11.0084,     10.8533,  ...,     11.7613,     11.6351,     11.4961],\n",
      "           [    10.7043,     10.6743,     10.5026,  ...,     11.4619,     11.4184,     11.3668],\n",
      "           ...,\n",
      "           [     4.8606,      4.4497,      3.9459,  ...,      5.2325,      5.2286,      5.2393],\n",
      "           [     5.0355,      4.5427,      4.1532,  ...,      5.6620,      5.7293,      5.7173],\n",
      "           [     5.0816,      4.7014,      4.4203,  ...,      6.0753,      6.3076,      6.2971]],\n",
      "\n",
      "          [[     3.9560,      4.1892,      4.3226,  ...,      2.8854,      2.8396,      3.0294],\n",
      "           [     4.2453,      4.4847,      4.5349,  ...,      3.6091,      3.6203,      3.7810],\n",
      "           [     4.4472,      4.6047,      4.8132,  ...,      4.1295,      4.3483,      4.7447],\n",
      "           ...,\n",
      "           [     0.2887,      0.3535,      0.3771,  ...,      1.1940,      1.2767,      1.4067],\n",
      "           [     0.6142,      0.6078,      0.7805,  ...,      1.7042,      1.8726,      1.9911],\n",
      "           [     1.1210,      1.1537,      1.2676,  ...,      2.4710,      2.5830,      2.6939]]]]], device='cuda:0'), 'q': tensor([[[[[    0.0329,     0.0326,     0.0323,  ...,     0.0346,     0.0344,     0.0342],\n",
      "           [    0.0330,     0.0329,     0.0329,  ...,     0.0348,     0.0349,     0.0348],\n",
      "           [    0.0333,     0.0332,     0.0333,  ...,     0.0352,     0.0353,     0.0349],\n",
      "           ...,\n",
      "           [    0.0260,     0.0261,     0.0265,  ...,     0.0271,     0.0274,     0.0278],\n",
      "           [    0.0267,     0.0267,     0.0271,  ...,     0.0277,     0.0280,     0.0281],\n",
      "           [    0.0274,     0.0273,     0.0275,  ...,     0.0281,     0.0284,     0.0286]],\n",
      "\n",
      "          [[    0.0327,     0.0325,     0.0322,  ...,     0.0349,     0.0350,     0.0352],\n",
      "           [    0.0323,     0.0324,     0.0325,  ...,     0.0346,     0.0350,     0.0351],\n",
      "           [    0.0321,     0.0322,     0.0324,  ...,     0.0343,     0.0347,     0.0346],\n",
      "           ...,\n",
      "           [    0.0233,     0.0236,     0.0239,  ...,     0.0225,     0.0227,     0.0227],\n",
      "           [    0.0232,     0.0233,     0.0236,  ...,     0.0223,     0.0225,     0.0224],\n",
      "           [    0.0233,     0.0233,     0.0235,  ...,     0.0222,     0.0225,     0.0225]],\n",
      "\n",
      "          [[    0.0262,     0.0258,     0.0253,  ...,     0.0308,     0.0304,     0.0303],\n",
      "           [    0.0264,     0.0263,     0.0262,  ...,     0.0311,     0.0311,     0.0310],\n",
      "           [    0.0270,     0.0270,     0.0270,  ...,     0.0320,     0.0322,     0.0319],\n",
      "           ...,\n",
      "           [    0.0201,     0.0201,     0.0201,  ...,     0.0221,     0.0218,     0.0214],\n",
      "           [    0.0205,     0.0205,     0.0206,  ...,     0.0221,     0.0218,     0.0214],\n",
      "           [    0.0208,     0.0209,     0.0210,  ...,     0.0221,     0.0220,     0.0217]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           ...,\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000]],\n",
      "\n",
      "          [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           ...,\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000]],\n",
      "\n",
      "          [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           ...,\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[    0.0329,     0.0325,     0.0323,  ...,     0.0348,     0.0346,     0.0345],\n",
      "           [    0.0329,     0.0328,     0.0328,  ...,     0.0351,     0.0351,     0.0351],\n",
      "           [    0.0332,     0.0331,     0.0333,  ...,     0.0354,     0.0355,     0.0351],\n",
      "           ...,\n",
      "           [    0.0263,     0.0264,     0.0268,  ...,     0.0272,     0.0275,     0.0279],\n",
      "           [    0.0270,     0.0269,     0.0273,  ...,     0.0278,     0.0281,     0.0281],\n",
      "           [    0.0275,     0.0274,     0.0277,  ...,     0.0281,     0.0284,     0.0286]],\n",
      "\n",
      "          [[    0.0326,     0.0325,     0.0323,  ...,     0.0351,     0.0353,     0.0354],\n",
      "           [    0.0321,     0.0322,     0.0324,  ...,     0.0348,     0.0352,     0.0354],\n",
      "           [    0.0318,     0.0319,     0.0322,  ...,     0.0345,     0.0349,     0.0348],\n",
      "           ...,\n",
      "           [    0.0234,     0.0237,     0.0240,  ...,     0.0231,     0.0233,     0.0234],\n",
      "           [    0.0233,     0.0234,     0.0237,  ...,     0.0229,     0.0231,     0.0231],\n",
      "           [    0.0234,     0.0233,     0.0235,  ...,     0.0229,     0.0231,     0.0231]],\n",
      "\n",
      "          [[    0.0252,     0.0248,     0.0243,  ...,     0.0308,     0.0304,     0.0303],\n",
      "           [    0.0253,     0.0252,     0.0251,  ...,     0.0311,     0.0311,     0.0310],\n",
      "           [    0.0259,     0.0258,     0.0258,  ...,     0.0320,     0.0323,     0.0320],\n",
      "           ...,\n",
      "           [    0.0201,     0.0201,     0.0201,  ...,     0.0222,     0.0219,     0.0215],\n",
      "           [    0.0205,     0.0205,     0.0205,  ...,     0.0222,     0.0219,     0.0215],\n",
      "           [    0.0209,     0.0209,     0.0210,  ...,     0.0222,     0.0221,     0.0218]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           ...,\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000]],\n",
      "\n",
      "          [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           ...,\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000]],\n",
      "\n",
      "          [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           ...,\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000],\n",
      "           [    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,     0.0000]]]]], device='cuda:0')}, metadata=Metadata(lat=tensor([71.7500, 71.5000, 71.2500, 71.0000, 70.7500, 70.5000, 70.2500, 70.0000, 69.7500, 69.5000, 69.2500, 69.0000, 68.7500, 68.5000, 68.2500, 68.0000, 67.7500, 67.5000, 67.2500, 67.0000, 66.7500,\n",
      "        66.5000, 66.2500, 66.0000, 65.7500, 65.5000, 65.2500, 65.0000, 64.7500, 64.5000, 64.2500, 64.0000, 63.7500, 63.5000, 63.2500, 63.0000, 62.7500, 62.5000, 62.2500, 62.0000, 61.7500, 61.5000,\n",
      "        61.2500, 61.0000, 60.7500, 60.5000, 60.2500, 60.0000, 59.7500, 59.5000, 59.2500, 59.0000, 58.7500, 58.5000, 58.2500, 58.0000, 57.7500, 57.5000, 57.2500, 57.0000, 56.7500, 56.5000, 56.2500,\n",
      "        56.0000, 55.7500, 55.5000, 55.2500, 55.0000, 54.7500, 54.5000, 54.2500, 54.0000, 53.7500, 53.5000, 53.2500, 53.0000, 52.7500, 52.5000, 52.2500, 52.0000, 51.7500, 51.5000, 51.2500, 51.0000,\n",
      "        50.7500, 50.5000, 50.2500, 50.0000, 49.7500, 49.5000, 49.2500, 49.0000, 48.7500, 48.5000, 48.2500, 48.0000, 47.7500, 47.5000, 47.2500, 47.0000, 46.7500, 46.5000, 46.2500, 46.0000, 45.7500,\n",
      "        45.5000, 45.2500, 45.0000, 44.7500, 44.5000, 44.2500, 44.0000, 43.7500, 43.5000, 43.2500, 43.0000, 42.7500, 42.5000, 42.2500, 42.0000, 41.7500, 41.5000, 41.2500, 41.0000, 40.7500, 40.5000,\n",
      "        40.2500, 40.0000, 39.7500, 39.5000, 39.2500, 39.0000, 38.7500, 38.5000, 38.2500, 38.0000, 37.7500, 37.5000, 37.2500, 37.0000, 36.7500, 36.5000, 36.2500, 36.0000, 35.7500, 35.5000, 35.2500,\n",
      "        35.0000, 34.7500, 34.5000, 34.2500, 34.0000, 33.7500, 33.5000, 33.2500, 33.0000, 32.7500, 32.5000, 32.2500, 32.0000], device='cuda:0'), lon=tensor([    0.0000,     0.2500,     0.5000,     0.7500,     1.0000,     1.2500,     1.5000,     1.7500,     2.0000,     2.2500,     2.5000,     2.7500,     3.0000,     3.2500,     3.5000,     3.7500,\n",
      "            4.0000,     4.2500,     4.5000,     4.7500,     5.0000,     5.2500,     5.5000,     5.7500,     6.0000,     6.2500,     6.5000,     6.7500,     7.0000,     7.2500,     7.5000,     7.7500,\n",
      "            8.0000,     8.2500,     8.5000,     8.7500,     9.0000,     9.2500,     9.5000,     9.7500,    10.0000,    10.2500,    10.5000,    10.7500,    11.0000,    11.2500,    11.5000,    11.7500,\n",
      "           12.0000,    12.2500,    12.5000,    12.7500,    13.0000,    13.2500,    13.5000,    13.7500,    14.0000,    14.2500,    14.5000,    14.7500,    15.0000,    15.2500,    15.5000,    15.7500,\n",
      "           16.0000,    16.2500,    16.5000,    16.7500,    17.0000,    17.2500,    17.5000,    17.7500,    18.0000,    18.2500,    18.5000,    18.7500,    19.0000,    19.2500,    19.5000,    19.7500,\n",
      "           20.0000,    20.2500,    20.5000,    20.7500,    21.0000,    21.2500,    21.5000,    21.7500,    22.0000,    22.2500,    22.5000,    22.7500,    23.0000,    23.2500,    23.5000,    23.7500,\n",
      "           24.0000,    24.2500,    24.5000,    24.7500,    25.0000,    25.2500,    25.5000,    25.7500,    26.0000,    26.2500,    26.5000,    26.7500,    27.0000,    27.2500,    27.5000,    27.7500,\n",
      "           28.0000,    28.2500,    28.5000,    28.7500,    29.0000,    29.2500,    29.5000,    29.7500,    30.0000,    30.2500,    30.5000,    30.7500,    31.0000,    31.2500,    31.5000,    31.7500,\n",
      "           32.0000,    32.2500,    32.5000,    32.7500,    33.0000,    33.2500,    33.5000,    33.7500,    34.0000,    34.2500,    34.5000,    34.7500,    35.0000,    35.2500,    35.5000,    35.7500,\n",
      "           36.0000,    36.2500,    36.5000,    36.7500,    37.0000,    37.2500,    37.5000,    37.7500,    38.0000,    38.2500,    38.5000,    38.7500,    39.0000,    39.2500,    39.5000,    39.7500,\n",
      "           40.0000,    40.2500,    40.5000,    40.7500,    41.0000,    41.2500,    41.5000,    41.7500,    42.0000,    42.2500,    42.5000,    42.7500,    43.0000,    43.2500,    43.5000,    43.7500,\n",
      "           44.0000,    44.2500,    44.5000,    44.7500,   335.0000,   335.2500,   335.5000,   335.7500,   336.0000,   336.2500,   336.5000,   336.7500,   337.0000,   337.2500,   337.5000,   337.7500,\n",
      "          338.0000,   338.2500,   338.5000,   338.7500,   339.0000,   339.2500,   339.5000,   339.7500,   340.0000,   340.2500,   340.5000,   340.7500,   341.0000,   341.2500,   341.5000,   341.7500,\n",
      "          342.0000,   342.2500,   342.5000,   342.7500,   343.0000,   343.2500,   343.5000,   343.7500,   344.0000,   344.2500,   344.5000,   344.7500,   345.0000,   345.2500,   345.5000,   345.7500,\n",
      "          346.0000,   346.2500,   346.5000,   346.7500,   347.0000,   347.2500,   347.5000,   347.7500,   348.0000,   348.2500,   348.5000,   348.7500,   349.0000,   349.2500,   349.5000,   349.7500,\n",
      "          350.0000,   350.2500,   350.5000,   350.7500,   351.0000,   351.2500,   351.5000,   351.7500,   352.0000,   352.2500,   352.5000,   352.7500,   353.0000,   353.2500,   353.5000,   353.7500,\n",
      "          354.0000,   354.2500,   354.5000,   354.7500,   355.0000,   355.2500,   355.5000,   355.7500,   356.0000,   356.2500,   356.5000,   356.7500,   357.0000,   357.2500,   357.5000,   357.7500,\n",
      "          358.0000,   358.2500,   358.5000,   358.7500,   359.0000,   359.2500,   359.5000,   359.7500], device='cuda:0'), time=(datetime.datetime(2000, 1, 1, 6, 0), datetime.datetime(2000, 2, 1, 6, 0)), atmos_levels=[1000.0, 925.0, 850.0, 700.0, 600.0, 500.0, 400.0, 300.0, 250.0, 200.0, 150.0, 100.0, 50.0], rollout_step=1))\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    for batch in test_dataloader:\n",
    "        print(\"Input timestamp\", batch.metadata.time)\n",
    "        output = model(batch)\n",
    "        print(\"Prediction timestamp\", output.metadata.time)\n",
    "        print(output)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 160, 280])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.surf_vars[\"2t\"].shape # t2m # THIS ONE WE NEED ! The temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input timestamp (datetime.datetime(2000, 1, 1, 0, 0), datetime.datetime(2000, 2, 1, 0, 0))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInput timestamp\u001b[39m\u001b[33m\"\u001b[39m, batch.metadata.time)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         preds = \u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction timestamp\u001b[39m\u001b[33m\"\u001b[39m, output.metadata.time)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/bfm-finetune/.venv/lib/python3.12/site-packages/aurora/rollout.py:42\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(model, batch, steps)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m pred\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Add the appropriate history so the model can be run on the prediction.\u001b[39;00m\n\u001b[32m     39\u001b[39m batch = dataclasses.replace(\n\u001b[32m     40\u001b[39m     pred,\n\u001b[32m     41\u001b[39m     surf_vars={\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         k: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msurf_vars\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pred.surf_vars.items()\n\u001b[32m     44\u001b[39m     },\n\u001b[32m     45\u001b[39m     atmos_vars={\n\u001b[32m     46\u001b[39m         k: torch.cat([batch.atmos_vars[k][:, \u001b[32m1\u001b[39m:], v], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pred.atmos_vars.items()\n\u001b[32m     48\u001b[39m     },\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    for batch in test_dataloader:\n",
    "        print(\"Input timestamp\", batch.metadata.time)\n",
    "        preds = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=2)]\n",
    "\n",
    "\n",
    "print(\"Prediction timestamp\", output.metadata.time)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
